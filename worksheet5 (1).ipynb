{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYxD1Tw1-ac4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Concept of AI -- week 5/student.csv')\n",
        "\n",
        "# 1. Print the first 5 and last 5 rows of the dataset\n",
        "\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(data.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jTj9rw2DnY6",
        "outputId": "43f1a8d9-bb74-402a-de9d-ca40e67a2e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bottom 5 rows of the dataset:\")\n",
        "print(data.tail(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cOfz9LNDqqC",
        "outputId": "63bf6e8b-57c5-4f33-c063-df881660e161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Print the information of the dataset\n",
        "print(\"Information about the dataset:\")\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHZ9x4hdIAX6",
        "outputId": "4a935147-f24d-4465-e6bf-801218b7e936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Gather descriptive statistics about the dataset\n",
        "print(\"Descriptive statistics of the dataset:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuo9f15mIJpR",
        "outputId": "e721ccfc-16a0-487a-f5d1-852ab1764269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive statistics of the dataset:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Split the dataset into Features (X) and Labels (Y)\n",
        "# Features: Math and Reading marks, Labels: Writing marks\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "print(\"Feature X:\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"Label Y:\")\n",
        "print(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4LBztvAISGt",
        "outputId": "05636e59-67d8-4960-fb27-0ea200690f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature X:\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "Label Y:\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Convert Features (X) and Labels (Y) into matrices\n",
        "X_matrix = X.to_numpy().T # Convert X to a numpy array and transpose it to shape (d, n)\n",
        "Y_matrix = Y.to_numpy()  #Convert Y to a numpy array with shape (n, )\n",
        "\n",
        "\n",
        "# Step 2: Create a weights matrix (W) initialized to zeros\n",
        "# Number of features = d (rows of X_matrix)\n",
        "num_features = X_matrix.shape[0]\n",
        "W_matrix = np.zeros((num_features,))\n",
        "\n",
        "# Print the matrices for verification\n",
        "print(\"Feature Matrix (X):\\n\", X_matrix)\n",
        "print(\"Label Matrix (Y):\\n\", Y_matrix)\n",
        "print(\"Weights Matrix (W):\\n\", W_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCNi8RhLI_En",
        "outputId": "5ffd2542-f696-43f7-b146-d744dc52dfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix (X):\n",
            " [[48 62 79 ... 89 83 66]\n",
            " [68 81 80 ... 87 82 66]]\n",
            "Label Matrix (Y):\n",
            " [ 63  72  78  79  62  85  83  41  80  77  64  90  45  77  70  46  76  44\n",
            "  85  72  53  66  75  49  84  83  68  66  77  78  74  83  72  65  46  66\n",
            "  50  79  68  46  86  70  61  53  72  75  50  77 100  81 100  87  78  48\n",
            "  50  44  48  43  67  78  58  91  92  78  42  85  73  83  61  58  60  55\n",
            "  48  62  68  59  62  48  74  63  80  79  73  79  45  67  89  77  81  88\n",
            "  53  68  79  77  63  73  60  67 100  79  26  51  80  57  41  78  68  49\n",
            "  76  41  71  77  89  86  55  80  56  74  85  80  73  74  86  56  53  44\n",
            "  41  59  71  81  74  78  67  53  56  75  82  79  99  76  59  96  75  61\n",
            "  56  88  65 100  79  55  61  83  74  59  54  47  82  74  59  74  84  59\n",
            "  43  65  61  78  84  73  73  92  63  72  61  59  70  87  78  65  73  62\n",
            "  69  55  73  63  67  86  78  85  83  80  60  90  56  70  55  80  82  60\n",
            "  78  76  94  75  68  71  85  46  58  46  84  58  57  59  77  63  68  99\n",
            "  48  91  57  80  46  75  59  87  82  79  66  68  66  61  66  63  72  73\n",
            "  77  84  83  42  72  76  76  39  74  43  63  74  52  31  65  45  87  63\n",
            "  51  82  86  76  27  70  79  66  61  62  47  17  65  76  75  66  59  61\n",
            "  93  40  66  43  71  64  55  86  65  70  65  53  49  67  76  95  76  48\n",
            "  60  53  69  78  62  66  51  52  46  42  77  57 100  84  68  48  72  50\n",
            "  72  55  72  77  56  94  67  82  75  80  60  73  74  62  53  69  75  60\n",
            "  58  71  87  74  87  73  78  76  74  55  94  71  76  59  91  57  83  59\n",
            "  93  64  58  79  96  76  64  70  80  33  95  64  92  34  72  81  57  79\n",
            "  84  82  54  45  54  62  49  74  59  63  83  62  72  72  65  65  54  78\n",
            "  82  85  74  83  71  83  77  66  75  52  68  84  67  70  41  91  46  58\n",
            "  67  70  83  64 100  49  77  57  67  80  74  41  67  59  86  88  57  80\n",
            "  58  52  31  84  97  71  62  58  71  41  66 100  51  35  81  94  72  38\n",
            "  82  79  55  75  90  95  65  39  85  86  54  93  69  84  78  58  73  60\n",
            "  44  67  69  55  59  88  42  78  84  68  66  51  43  38  69  90  73  67\n",
            "  57  81  63  80  78  65  74  80  60  60  63  64  72  51  71  63  82  76\n",
            "  39  79  48  70  90  73  58 100  80  75  72  79  52  56  65  45  59  61\n",
            "  47  62  83  90  76  72  69  57  56  40  79  48  57  47  78  45  74  69\n",
            "  59  85  45  54  72  74  75  55  49  53  83  22 100  67  83  46  43  74\n",
            "  64  35  67  87  77  91  74  96  82  78  73  52  91  66  67  71  74  71\n",
            "  61  47  76  85  93  41  81  86  53  91  68  96  48  71  75  72  71  62\n",
            "  67  53  74  63  82  57  69  52  91  73  73  75  36  71  62 100  50  74\n",
            "  60  75  83  83 100  67  71  77  67  95  52  71  74  60  67  79  75  95\n",
            "  69  80  48  61  82  39  70  70  69  32  79  53  59  83 100  80  80  82\n",
            "  56  83  85  88  81  95  63  70  89  59  56  62  95  63  82  69  58  74\n",
            "  66  82  94  70  78  63  91  70  62  79  65  74  56  65 100  70  66  54\n",
            "  72  90  56  65  50  95  38  76  84  76  55  85  70  73  80  83  53  67\n",
            " 100  67  44  96  48  77 100  40  91  55  41  25  63  59  63  77  46  49\n",
            "  46  93  39  58  87  57  77 100  65  34  87  81  63  69  74  70  93  63\n",
            "  81  81  63  87  76  54  89  63  76  79  75  50  36  82  83  85  82  41\n",
            "  82  45  57  88  81  98  61  95  84  71  52  71  90  75  62  63  86  70\n",
            "  77  68  80  67  67  89  60  79  80  78  70  72  43  14  54  92  71  65\n",
            "  58  56  67  64  81  55  45  86  52  75  81  62  42  21  72  55  66  69\n",
            "  86  67  78  85  66  47 100  63  62  61  69  57  76  52  47  51  61  45\n",
            "  59  81  65  53  61  90  74  62  67  50  84  70  52  92  65  65  67  72\n",
            "  66  62  99  62  53  57  78  56  87  79  63  87  86  75  70  60  49  41\n",
            "  78  58  75  89  34  60  80  85  73  58  69  74  52  58  79  86  61  68\n",
            "  67  48  65  73  57  73  57  80  85  81  61  69 100  99  92  72  57  44\n",
            "  59  62  93  64  57  72  40  85  60  83  63  74  44  61  74  68  78  50\n",
            "  70  68  82  46  96 100  44  41  95  79  67  52  87  75  61  42  60  57\n",
            "  64  52  68  58  93  75  77  66  63  90  43  65  95  86  31  95  52  63\n",
            "  87  70  59  84  79  77  75  66  69  85  63  50  58  80  47  55  61  87\n",
            "  77  54  66  68  54  69  74  81  72  61  76  63  64  73  62  92  69  70\n",
            "  65  53  74  61  80  85  62  80  83  56  76  52  51  74  57  63  61  87\n",
            "  60  54  89  67  56  70  90  94  78  72]\n",
            "Weights Matrix (W):\n",
            " [0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Step 2: Verify the split\n",
        "print(\"\\nTraining Features (X_train):\")\n",
        "print(X_train.head())\n",
        "\n",
        "print(\"\\nTraining Labels (Y_train):\")\n",
        "print(Y_train.head())\n",
        "\n",
        "print(\"\\nTesting Features (X_test):\")\n",
        "print(X_test.head())\n",
        "\n",
        "print(\"\\nTesting Labels (Y_test):\")\n",
        "print(Y_test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmw9pwQgKOkE",
        "outputId": "4bea770a-2805-4012-ffbc-812b6eda826b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Features (X_train):\n",
            "     Math  Reading\n",
            "29     64       82\n",
            "535    62       70\n",
            "695    36       21\n",
            "557    81       70\n",
            "836    82       86\n",
            "\n",
            "Training Labels (Y_train):\n",
            "29     78\n",
            "535    67\n",
            "695    25\n",
            "557    71\n",
            "836    87\n",
            "Name: Writing, dtype: int64\n",
            "\n",
            "Testing Features (X_test):\n",
            "     Math  Reading\n",
            "521    63       69\n",
            "737    42       37\n",
            "740    69       62\n",
            "660    69       59\n",
            "411   100       92\n",
            "\n",
            "Testing Labels (Y_test):\n",
            "521    69\n",
            "737    41\n",
            "740    57\n",
            "660    56\n",
            "411    88\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "  \"\"\" Parameters:\n",
        "  This function finds the Mean Square Error.\n",
        "  Input parameters:\n",
        "  X: Feature Matrix\n",
        "  Y: Target Matrix\n",
        "  W: Weight Matrix\n",
        "  Output Parameters:\n",
        "  cost: accumulated mean square error.\n",
        "  \"\"\"\n",
        "  n = len(Y)\n",
        "  Y_pred = np.dot(X, W)\n",
        "  cost = (1/n) * np.sum((Y_pred - Y) ** 2)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "mM2GSodQKtXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize weights.\n",
        "  Parameters:\n",
        "  X: Feature matrix (n_samples x n_features)\n",
        "  Y: Target vector (n_samples)\n",
        "  W: Weight vector (n_features)\n",
        "  alpha: Learning rate\n",
        "  iterations: Number of iterations\n",
        "\n",
        "  Returns:\n",
        "  W_update: Optimized weights\n",
        "  cost_history: History of cost function values\n",
        "  \"\"\"\n",
        "  cost_history = []\n",
        "  m = len(Y)\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # Step 1: Compute predictions\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    # Step 2: Compute gradient\n",
        "    gradient = (1/m) * np.dot(X.T, (Y_pred - Y))\n",
        "\n",
        "    # Step 3: Update weights\n",
        "    W -= alpha * gradient\n",
        "\n",
        "    # Step 4: Compute and store cost\n",
        "    cost = cost_function(X, Y, W)\n",
        "    cost_history.append(cost)\n",
        "\n",
        "  return W, cost_history\n"
      ],
      "metadata": {
        "id": "m-SyziLGQODk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE Calculation\n",
        "def rmse(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  Calculate Root Mean Squared Error.\n",
        "  \"\"\"\n",
        "  return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# R-Squared Calculation\n",
        "def r2(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  Calculate R-squared value.\n",
        "  \"\"\"\n",
        "\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "  ss_red = np.sum((Y - Y_pred) ** 2)\n",
        "  return 1 - (ss_red / ss_tot)"
      ],
      "metadata": {
        "id": "0Zbgc33JTXCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  data = pd.read_csv('/content/drive/MyDrive/Concept of AI -- week 5/student.csv')\n",
        "\n",
        "  # Step 2: Split the data into features (X) and target (Y)\n",
        "  X = data[['Math', 'Reading']].values\n",
        "  Y = data['Writing'].values\n",
        "\n",
        "  #Step 3: Split the data into training and test sets\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)\n",
        "\n",
        "  # Step 4: Initialize weights (W), learning rate, and iterations\n",
        "  W = np.zeros(X.shape[1])\n",
        "  alpha = 0.00001\n",
        "  iterations = 1000\n",
        "\n",
        "  # Step 5: Perform Gradient Descent\n",
        "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "  # Step 6: Make predictions on the test set\n",
        "  Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "  # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "  model_rmse = rmse(Y_test, Y_pred)\n",
        "  model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "  print(\"Final Weights:\", W_optimal)\n",
        "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "  print(\"RMSE on the Set:\", model_rmse )\n",
        "  print(\"R-squared on the Set:\",model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTEhNyaaUNNO",
        "outputId": "a33c438f-208f-4e16-97ce-d7b4a4648733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [4026.33114156751, 3280.573665199384, 2674.1239989803175, 2180.9589785701155, 1779.9166540166468, 1453.788198601909, 1188.5794521617188, 972.910410590327, 797.5268927198968, 654.9034294649376]\n",
            "RMSE on the Set: 5.2798239764188635\n",
            "R-squared on the Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model Performance: Overfitting, Underfitting, or Acceptable Performance?\n",
        "Key Metrics to Evaluate Performance:\n",
        "\n",
        "Root Mean Square Error (RMSE):\n",
        "\n",
        "-- Low RMSE indicates the model predicts the target values well.\n",
        "\n",
        "R-squared (R²):\n",
        "\n",
        "-- A value close to 1 implies the model explains most of the variance in the target variable.\n",
        "\n",
        "Scenario Evaluation:\n",
        "\n",
        "- Overfitting:\n",
        "\n",
        "-- If R² on the training set is significantly higher than R² on the test set, the model memorizes the training data and fails to generalize.\n",
        "\n",
        "- Underfitting:\n",
        "\n",
        "-- Both training and test R² are low, indicating the model is too simple to capture the relationship.\n",
        "\n",
        "- Acceptable Performance:\n",
        "\n",
        "-- The model achieves a similar R² score for both training and test sets, with a low RMSE.\n",
        "\n",
        "Observation from the Results:\n",
        "\n",
        "-- If the RMSE on the test set is small (e.g., ~4.57) and the R² is high (~0.85), it suggests the model performs well and generalizes effectively.\n",
        "Comparing training and test performance metrics (which can be included in the main function) will confirm whether the model overfits, underfits, or is acceptable."
      ],
      "metadata": {
        "id": "vK9HsdmhW2Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Experimenting with Learning Rate:\n",
        "Impact of Learning Rate (α):\n",
        "\n",
        "Higher Learning Rate:\n",
        "Convergence may happen faster, but too high a learning rate can cause the model to diverge or oscillate.\n",
        "Lower Learning Rate: The model converges slowly and may require more iterations, but it ensures stability.\n",
        "Experimentation:\n",
        "\n",
        "Learning Rate (α) = 0.1: Likely to converge faster but might overshoot the minimum or diverge.\n",
        "Learning Rate (α) = 0.0001: Likely stable but may take many iterations to converge.\n",
        "Default Learning Rate (α = 0.00001): A balanced learning rate providing stable and accurate convergence."
      ],
      "metadata": {
        "id": "e6k2MydhXsd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "iterations = 1000  # Define the number of iterations for gradient descent\n",
        "\n",
        "# Experiment with Learning Rates\n",
        "for alpha in [0.1, 0.01, 0.0001, 0.00001]:\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, np.zeros(X_train.shape[1]), alpha, iterations)\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "    print(f\"Learning Rate: {alpha}\")\n",
        "    print(f\"RMSE on Test Set: {model_rmse}\")\n",
        "    print(f\"R-Squared on Test Set: {model_r2}\")\n",
        "    print(\"-\" * 40)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF1-hGZ7WQ3f",
        "outputId": "c35ec275-b782-482f-cd4d-a10e04e33e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-e64a3002b6cd>:26: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: 1.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "<ipython-input-15-e64a3002b6cd>:26: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.01\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: 1.0\n",
            "----------------------------------------\n",
            "Learning Rate: 0.0001\n",
            "RMSE on Test Set: 4.792607360540954\n",
            "R-Squared on Test Set: 0.908240340333986\n",
            "----------------------------------------\n",
            "Learning Rate: 1e-05\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTY2LbEOX1VR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}